{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом ноуте - набросок того, как можно получать confidence score результата модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Tuple, Dict, Union\n",
    "\n",
    "\n",
    "class TransformerSpellChecker:\n",
    "    \"\"\"\n",
    "    Класс для проверки орфографии и пунктуации с использованием модели на основе Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"ai-forever/sage-fredt5-distilled-95m\",\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация токенизатора и модели.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Название предобученной модели.\n",
    "            device (str): Устройство для вычислений (\"cuda\" или \"cpu\").\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def predict_verbose(\n",
    "        self, text: str\n",
    "    ) -> Tuple[List[Dict[str, Union[str, float]]], str]:\n",
    "        \"\"\"\n",
    "        Возвращает исправленный текст и список всех предложенных исправлений с оценками уверенности.\n",
    "\n",
    "        Args:\n",
    "            text (str): Входной текст для проверки.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[Dict[str, Union[str, float]]], str]: Исправленный текст и список всех предложенных исправлений с оценками уверенности.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=False, padding=True\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs.to(self.device),\n",
    "                max_length=inputs[\"input_ids\"].size(1) * 1.5,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "\n",
    "        # Декодируем сгенерированные токены в строки\n",
    "        generated_text = self.tokenizer.batch_decode(\n",
    "            outputs.sequences, skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        # Генерируем список исправлений с уверенности для каждого исправления\n",
    "        corrections = self._generate_corrections_with_confidence(\n",
    "            text, generated_text, outputs.scores\n",
    "        )\n",
    "\n",
    "        return corrections, generated_text\n",
    "\n",
    "    def _generate_corrections_with_confidence(\n",
    "        self, original_text: str, corrected_text: str, logits: List[torch.Tensor]\n",
    "    ) -> List[Dict[str, Union[str, float]]]:\n",
    "        \"\"\"\n",
    "        Генерирует список исправлений с оценками уверенности на основе оригинального и сгенерированного текста.\n",
    "\n",
    "        Args:\n",
    "            original_text (str): Оригинальный текст.\n",
    "            corrected_text (str): Исправленный текст.\n",
    "            logits (List[torch.Tensor]): Логиты модели для каждого токена.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Union[str, float]]]: Список исправлений с оценками уверенности.\n",
    "        \"\"\"\n",
    "        corrections = []\n",
    "\n",
    "        # Инициализация счетчиков для отслеживания позиции в тексте\n",
    "        orig_idx = 0\n",
    "        corr_idx = 0\n",
    "\n",
    "        while orig_idx < len(original_text) and corr_idx < len(corrected_text):\n",
    "            if original_text[orig_idx] == corrected_text[corr_idx]:\n",
    "                # Если символы совпадают, переходим к следующему\n",
    "                orig_idx += 1\n",
    "                corr_idx += 1\n",
    "            else:\n",
    "                # Ищем исправленное слово\n",
    "                orig_token_end = orig_idx\n",
    "                while (\n",
    "                    orig_token_end < len(original_text)\n",
    "                    and original_text[orig_token_end] != \" \"\n",
    "                ):\n",
    "                    orig_token_end += 1\n",
    "                original_word = original_text[orig_idx:orig_token_end]\n",
    "\n",
    "                corr_token_end = corr_idx\n",
    "                while (\n",
    "                    corr_token_end < len(corrected_text)\n",
    "                    and corrected_text[corr_token_end] != \" \"\n",
    "                ):\n",
    "                    corr_token_end += 1\n",
    "                corrected_word = corrected_text[corr_idx:corr_token_end]\n",
    "\n",
    "                # Получаем уверенность по логитам\n",
    "                if len(logits) > corr_idx:\n",
    "                    logits_for_token = logits[corr_idx]\n",
    "                    probabilities = F.softmax(logits_for_token[0], dim=-1)\n",
    "                    token_id = self.tokenizer.convert_tokens_to_ids(\n",
    "                        self.tokenizer.tokenize(corrected_word)\n",
    "                    )\n",
    "                    confidence_score = probabilities[token_id[0]].item()\n",
    "                else:\n",
    "                    confidence_score = 0.0\n",
    "\n",
    "                # Добавляем исправление\n",
    "                corrections.append(\n",
    "                    {\n",
    "                        \"index\": orig_idx,\n",
    "                        \"error\": original_word,\n",
    "                        \"suggestions\": [corrected_word],\n",
    "                        \"confidence\": confidence_score,\n",
    "                        \"message\": f\"Исправлено с вероятностью {confidence_score:.2f}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Продвигаем индексы\n",
    "                orig_idx = orig_token_end + 1\n",
    "                corr_idx = corr_token_end + 1\n",
    "\n",
    "        return corrections\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Возвращает исправленный текст.\n",
    "\n",
    "        Args:\n",
    "            text (str): Входной текст для проверки.\n",
    "\n",
    "        Returns:\n",
    "            str: Исправленный текст.\n",
    "        \"\"\"\n",
    "        _, corrected_text = self.predict_verbose(text)\n",
    "        return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerSpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'index': 0,\n",
       "   'error': 'превет',\n",
       "   'suggestions': ['Привет,'],\n",
       "   'confidence': 0.9987919926643372,\n",
       "   'message': 'Исправлено с вероятностью 1.00'},\n",
       "  {'index': 9,\n",
       "   'error': 'ондрей',\n",
       "   'suggestions': ['Андрей.'],\n",
       "   'confidence': 0.0,\n",
       "   'message': 'Исправлено с вероятностью 0.00'}],\n",
       " 'Привет, я Андрей.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_verbose(\"превет я ондрей ,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
